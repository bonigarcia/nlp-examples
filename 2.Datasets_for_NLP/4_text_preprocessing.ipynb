{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_text_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1XUz3f4F9-4"
      },
      "source": [
        "**Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjUz7ar8l1Ez",
        "outputId": "10e2f3ff-f972-44b7-c0d7-4c500ff70fb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "my_message = \"@Everybody: Hello NLP-world!\"\n",
        "word_tokenize(my_message)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@', 'Everybody', ':', 'Hello', 'NLP-world', '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "693oQ6X1nUFX",
        "outputId": "405171dc-2571-4bc0-d846-fa427a1237bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "\n",
        "wordpunct_tokenize(my_message)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@', 'Everybody', ':', 'Hello', 'NLP', '-', 'world', '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxS7HjZun4kh",
        "outputId": "b10925d5-829d-4ce7-9614-7f12840609dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.tokenize import regexp_tokenize\n",
        "\n",
        "regexp_tokenize(my_message, \"\\w+|[!,\\-,]\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Everybody', 'Hello', 'NLP', '-', 'world', '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmUC3n1rpbAZ"
      },
      "source": [
        "**Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YE64x4spih6",
        "outputId": "6d5970e9-e09e-4344-e13e-7442e328726c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemming = PorterStemmer()\n",
        "\n",
        "word_list = [\"Enjoy\", \"enjoying\", \"enjoys\", \"enjoyable\"]\n",
        "for word in word_list:\n",
        "    print(stemming.stem(word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "enjoy\n",
            "enjoy\n",
            "enjoy\n",
            "enjoy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSjbRREXq0fW"
      },
      "source": [
        "**Removing stop words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju-QZPcYq205",
        "outputId": "830f3289-1671-4ff0-99c8-1986618c15b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "sw_en = stopwords.words(\"english\")\n",
        "example_text = \"This is an example sentence to test stopwords\"\n",
        "example_text_without_stopwords = [word for word in example_text.split() if word not in sw_en]\n",
        "print(example_text)\n",
        "print(example_text_without_stopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "This is an example sentence to test stopwords\n",
            "['This', 'example', 'sentence', 'test', 'stopwords']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TokbWarcvADn",
        "outputId": "1b4b8677-dd0e-4d85-f9c3-da137de794a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "input_list = [\"red\", \"white\", \"purple\", \"yellow\", \"blue\", \"green\", \"black\"]\n",
        "\n",
        "# Example non-list comprehesion\n",
        "output_list = []\n",
        "for item in input_list:\n",
        "    if \"u\" in item:\n",
        "      output_list.append(item)\n",
        "print(output_list)\n",
        "\n",
        "# Example of list-comprehension\n",
        "output_list = [item for item in input_list if \"u\" in item]\n",
        "print(output_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['purple', 'blue']\n",
            "['purple', 'blue']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5dCifvIsHBv",
        "outputId": "ff8661e7-3ecd-4f9e-91f7-19f957b01439",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.corpus import gutenberg\n",
        "nltk.download(\"gutenberg\")\n",
        "\n",
        "words_in_hamlet = gutenberg.words(\"shakespeare-hamlet.txt\")\n",
        "words_in_hamlet_without_stopwords = [word for word in words_in_hamlet if word not in sw_en]\n",
        "\n",
        "stopwords_percentage = len(words_in_hamlet_without_stopwords) * 100 / len(words_in_hamlet)\n",
        "print(\"The percentage of words without stopwords in Hamlet is\", stopwords_percentage, \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "The percentage of words without stopwords in Hamlet is 69.26124197002142 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPM-J7jCWayU"
      },
      "source": [
        "**Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cs0oJ25Az2Bt",
        "outputId": "dcf58c63-bf76-4962-bae2-46e0d40db112",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en\")\n",
        "\n",
        "input_str=\"been had done languages cities mice\"\n",
        "output_list = [token.lemma_ for token in nlp(input_str)]\n",
        "print(output_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['be', 'have', 'do', 'language', 'city', 'mouse']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}