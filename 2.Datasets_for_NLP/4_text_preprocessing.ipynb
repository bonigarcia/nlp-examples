{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_text_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1XUz3f4F9-4"
      },
      "source": [
        "**Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjUz7ar8l1Ez",
        "outputId": "8a537ab3-b16b-498a-d7f4-2fd3916a704f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "my_message = \"@Everybody: Hello NLP-world!\"\n",
        "word_tokenize(my_message)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@', 'Everybody', ':', 'Hello', 'NLP-world', '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "693oQ6X1nUFX",
        "outputId": "299e04ab-7d73-465e-be34-24573cf094a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "\n",
        "wordpunct_tokenize(my_message)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@', 'Everybody', ':', 'Hello', 'NLP', '-', 'world', '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxS7HjZun4kh",
        "outputId": "f3622d42-d7c4-4583-9b57-2385a269212d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.tokenize import regexp_tokenize\n",
        "\n",
        "regexp_tokenize(my_message, r\"\\w+\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Everybody', 'Hello', 'NLP', 'world']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmUC3n1rpbAZ"
      },
      "source": [
        "**Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YE64x4spih6",
        "outputId": "309fc462-a775-468f-ebaa-e57e83becec5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "tokens = [\"Enjoy\", \"enjoying\", \"enjoys\", \"enjoyable\"]\n",
        "stems = [stemmer.stem(token) for token in tokens]\n",
        "print(stems)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['enjoy', 'enjoy', 'enjoy', 'enjoy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_Mdv8wEtEB5",
        "outputId": "4e7f507c-75a9-4a61-827c-55d6e0d8d1a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "stems = [stemmer.stem(token) for token in tokens]\n",
        "print(stems)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['enjoy', 'enjoy', 'enjoy', 'enjoy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSjbRREXq0fW"
      },
      "source": [
        "**Removing stop words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju-QZPcYq205",
        "outputId": "ab808da4-5d15-4cfa-9fe5-3b1ba032d1eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "example_text = \"This is an example sentence to test stopwords\"\n",
        "sw_en = stopwords.words(\"english\")\n",
        "\n",
        "text_no_stopwords = [word for word in example_text.split() if word not in sw_en]\n",
        "print(example_text)\n",
        "print(text_no_stopwords)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "This is an example sentence to test stopwords\n",
            "['This', 'example', 'sentence', 'test', 'stopwords']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TokbWarcvADn",
        "outputId": "1b4b8677-dd0e-4d85-f9c3-da137de794a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "input_list = [\"red\", \"white\", \"purple\", \"yellow\", \"blue\", \"green\", \"black\"]\n",
        "\n",
        "# Example non-list comprehesion\n",
        "output_list = []\n",
        "for item in input_list:\n",
        "    if \"u\" in item:\n",
        "      output_list.append(item)\n",
        "print(output_list)\n",
        "\n",
        "# Example of list-comprehension\n",
        "output_list = [item for item in input_list if \"u\" in item]\n",
        "print(output_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['purple', 'blue']\n",
            "['purple', 'blue']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5dCifvIsHBv",
        "outputId": "dacd00e9-1f5a-422d-badd-a5a67fd87676",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.corpus import gutenberg\n",
        "nltk.download(\"gutenberg\")\n",
        "\n",
        "words = gutenberg.words(\"shakespeare-hamlet.txt\")\n",
        "words_no_stopwords = [word for word in words if word not in sw_en]\n",
        "\n",
        "stopwords_percentage = len(text_no_stopwords) * 100 / len(words)\n",
        "print(\"The percentage of words without stopwords in Hamlet is\", stopwords_percentage, \"%\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "The percentage of words without stopwords in Hamlet is 0.013383297644539615 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPM-J7jCWayU"
      },
      "source": [
        "**Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cs0oJ25Az2Bt",
        "outputId": "281b591c-7d81-4ec3-9b40-d2e544df78e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en\")\n",
        "\n",
        "sentence=\"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
        "\n",
        "lemmas = [token.lemma_ for token in nlp(sentence)]\n",
        "print(lemmas)\n",
        "\n",
        "lemmas = [w.lemma_ if w.lemma_ !='-PRON-' else w.text for w in nlp(sentence)]\n",
        "print(lemmas)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['-PRON-', 'be', 'put', 'in', 'effort', 'to', 'enhance', '-PRON-', 'understanding', 'of', 'lemmatization']\n",
            "['We', 'be', 'put', 'in', 'effort', 'to', 'enhance', 'our', 'understanding', 'of', 'lemmatization']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}