{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_text_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1XUz3f4F9-4"
      },
      "source": [
        "**Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcY1sleuaVR_",
        "outputId": "d3938164-8442-44a5-808f-0ef75848dd0d"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "my_message = \"Hello there. Goodbye everybody.\"\n",
        "tokens = sent_tokenize(my_message)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello there.', 'Goodbye everybody.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjUz7ar8l1Ez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c1a257b-d90c-4b7a-d1a8-3ad1a568c2b3"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "my_message = \"@Everybody: Hello NLP-world!\"\n",
        "tokens = word_tokenize(my_message)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['@', 'Everybody', ':', 'Hello', 'NLP-world', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "693oQ6X1nUFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "332eac5e-3518-48ed-dbb9-d746a9c840c9"
      },
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "\n",
        "tokens = wordpunct_tokenize(my_message)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['@', 'Everybody', ':', 'Hello', 'NLP', '-', 'world', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxS7HjZun4kh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6cf1f8e-2356-4b62-874d-86aa89712157"
      },
      "source": [
        "from nltk.tokenize import regexp_tokenize\n",
        "\n",
        "tokens = regexp_tokenize(my_message, r\"\\w+\")\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Everybody', 'Hello', 'NLP', 'world']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ota288o3fDI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a897cb9-bde4-4acb-d8ea-e630f148de38"
      },
      "source": [
        "from nltk.tokenize import regexp_tokenize\n",
        "\n",
        "input_sentences = [\"Hello world\", \"this is only an example\"]\n",
        "tokens = []\n",
        "for word in input_sentences:\n",
        "   tokens.extend(regexp_tokenize(word, r\"\\w+\"))\n",
        "\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello', 'world', 'this', 'is', 'only', 'an', 'example']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmUC3n1rpbAZ"
      },
      "source": [
        "**Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YE64x4spih6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "309fc462-a775-468f-ebaa-e57e83becec5"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "tokens = [\"Enjoy\", \"enjoying\", \"enjoys\", \"enjoyable\"]\n",
        "stems = [stemmer.stem(token) for token in tokens]\n",
        "print(stems)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['enjoy', 'enjoy', 'enjoy', 'enjoy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_Mdv8wEtEB5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e7f507c-75a9-4a61-827c-55d6e0d8d1a0"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "stems = [stemmer.stem(token) for token in tokens]\n",
        "print(stems)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['enjoy', 'enjoy', 'enjoy', 'enjoy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD2hCfsdtd0g"
      },
      "source": [
        "List-comprehension in Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TokbWarcvADn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8347f3d7-c5c5-4f3a-d4a6-aadeca3b5437"
      },
      "source": [
        "input_list = [\"red\", \"white\", \"purple\", \"yellow\", \"blue\", \"green\", \"black\"]\n",
        "\n",
        "# Example non-list comprehesion\n",
        "output_list = []\n",
        "for item in input_list:\n",
        "    output_list.append(item)\n",
        "print(output_list)\n",
        "\n",
        "# Example of list-comprehension\n",
        "output_list = [item for item in input_list]\n",
        "print(output_list)\n",
        "\n",
        "# Example non-list comprehesion (with conditional)\n",
        "output_list = []\n",
        "for item in input_list:\n",
        "    if \"u\" in item:\n",
        "      output_list.append(item)\n",
        "print(output_list)\n",
        "\n",
        "# Example of list-comprehension (with conditional)\n",
        "output_list = [item for item in input_list if \"u\" in item]\n",
        "print(output_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['red', 'white', 'purple', 'yellow', 'blue', 'green', 'black']\n",
            "['red', 'white', 'purple', 'yellow', 'blue', 'green', 'black']\n",
            "['purple', 'blue']\n",
            "['purple', 'blue']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSjbRREXq0fW"
      },
      "source": [
        "**Removing stop words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju-QZPcYq205",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab808da4-5d15-4cfa-9fe5-3b1ba032d1eb"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "example_text = \"This is an example sentence to test stopwords\"\n",
        "sw_en = stopwords.words(\"english\")\n",
        "\n",
        "text_no_stopwords = [word for word in example_text.split() if word not in sw_en]\n",
        "print(example_text)\n",
        "print(text_no_stopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "This is an example sentence to test stopwords\n",
            "['This', 'example', 'sentence', 'test', 'stopwords']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5dCifvIsHBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dacd00e9-1f5a-422d-badd-a5a67fd87676"
      },
      "source": [
        "from nltk.corpus import gutenberg\n",
        "nltk.download(\"gutenberg\")\n",
        "\n",
        "words = gutenberg.words(\"shakespeare-hamlet.txt\")\n",
        "words_no_stopwords = [word for word in words if word not in sw_en]\n",
        "\n",
        "stopwords_percentage = len(text_no_stopwords) * 100 / len(words)\n",
        "print(\"The percentage of words without stopwords in Hamlet is\", stopwords_percentage, \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "The percentage of words without stopwords in Hamlet is 0.013383297644539615 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPM-J7jCWayU"
      },
      "source": [
        "**Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cs0oJ25Az2Bt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a767f49-a2cd-4ec5-847a-0c5603975889"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en\")\n",
        "\n",
        "sentence = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
        "\n",
        "lemmas = [token.lemma_ for token in nlp(sentence)]\n",
        "print(lemmas)\n",
        "\n",
        "lemmas = [w.lemma_ if w.lemma_ !='-PRON-' else w.text for w in nlp(sentence)]\n",
        "print(lemmas)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['-PRON-', 'be', 'put', 'in', 'effort', 'to', 'enhance', '-PRON-', 'understanding', 'of', 'lemmatization']\n",
            "['We', 'be', 'put', 'in', 'effort', 'to', 'enhance', 'our', 'understanding', 'of', 'lemmatization']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuT4ZTZgQ-Nf"
      },
      "source": [
        "**POS tagging**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7uji_APRABp",
        "outputId": "d6b37b7a-d043-4870-c491-828aca02fb66"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "\n",
        "sentence = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tagged = pos_tag(tokens)\n",
        "\n",
        "print(\"tokens\", tokens)\n",
        "print(\"pos_tagged\", pos_tagged)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "tokens ['We', 'are', 'putting', 'in', 'efforts', 'to', 'enhance', 'our', 'understanding', 'of', 'Lemmatization']\n",
            "pos_tagged [('We', 'PRP'), ('are', 'VBP'), ('putting', 'VBG'), ('in', 'IN'), ('efforts', 'NNS'), ('to', 'TO'), ('enhance', 'VB'), ('our', 'PRP$'), ('understanding', 'NN'), ('of', 'IN'), ('Lemmatization', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dI-WRC0S5WS",
        "outputId": "cc8539c0-3d7f-44a9-e056-e15c9d61e853"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "# Return tag compliance to WordNet lemmatization (a, n, r, v) \n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN # Nouns by default\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in pos_tagged]\n",
        "\n",
        "print(\"lemmas\", lemmas)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "lemmas ['We', 'be', 'put', 'in', 'effort', 'to', 'enhance', 'our', 'understanding', 'of', 'Lemmatization']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}